1.hadoop的环境搭建
	在hadoop-env.sh脚本里指定JAVA_HOME
	在core-site.xml文件里配置文件系统的url:fs.defaultFS:hdfs://bg:9000
	在HDFS-site.xml里配置文件块的复制数量:dfs.replication:1
	设置ssh本机无密码登陆
	tail -f 查看只读文件
	
2.配置模板
	<configuration>
		<property>	#
			<name>hbase.rootdir</name>
			<value>file:///home/testuser/hbase</value>
		</property>
		<property>
			<name>hbase.zookeeper.property.dataDir</name>
			<value>/home/testuser/zookeeper</value>
		</property>
	</configuration>

3.常用配置项:
	core-site.xml:
		fs.defaultFS					hdfs://bg:9000		这里的值指的是默认的HDFS路径。当有多个HDFS集群同时工作时，用户如果不写集群名称，那么默认使用哪个哪？在这里指定！该值来自于hdfs-site.xml中的配置
		hadoop.tmp.dir					/usr/local/tmp		这里的路径默认是NameNode、DataNode、JournalNode等存放数据的公共目录。用户也可以自己单独指定这三类节点的目录
	hdfs-site.xml:
		dfs.replication					1					指定数据备份几份
		dfs.namenode.http-address		bg:50070			namenode的监听端口(提供web服务)
		fs.namenode.name.dir			/usr/local/name		NameNode节点的数据存放目录
		dfs.datanode.data.dir			/usr/local/data		DataNode节点的数据存放目录
		ha.zookeeper.quorum				
		
		
4.CDH版本安装
	1.关闭防火墙
		service iptables stop
		chkconfig --list|grep iptables 查看iptables是否开机自启
		chkconfig iptables off 关闭iptables的开机自启
	2.关闭SELINUX
		vim /etc/sysconfig/selinux		打开selinux配置文件
		SELINUX=disabled #增加			关闭selinux
	3.配置hosts
	4.修改主机名
		hostname bg	修改主机名为bg
		vim /etc/sysconfig/network	打开包含主机名的配置文件
	5.卸载自带的jdk
		rpm -qa|grep jdk	查看是否安装了jdk
		rpm -e 程序名		卸载
	6.添加hadoop用户
		adduser hadoop	创建hadopp用户
		passwd	hadoop	设置hadoop密码
		whoami 查看当前登陆的用户
		vim /etc/sudoers	权限配置文件
		hadoop  ALL=(root)NOPASSWD:ALL 设置hadoop用户无密码使用root权限
	7.设置ssh无密钥登陆
		ssh-key -t rsa 生成ssh密钥
		ssh-copy-id 主机名	拷贝ssh公钥到指定主机

5.YARN
	hadoop各配置文件的作用
	core-site.xml	通用属性
	hdfs-site.xml	HDFS属性
	mapred-site.xml	MapReduce属性
	yarn-site.xml	Yarn属性
	
	安装环境:
		Linux
		Hadoop
		JDK
		OpenSSH
		
6.用到的命令
	addgroup hadoop 创建hadoop用户组
	adduser --ingroup hadoop hduser 创建hduser用户并把它加入hadoop用户组中
	which java 查看java命令所在的路径
	
7.ZooKeeper
	版本:
		version 数据节点内容的版本号
		cversion 当前数据节点子节点的版本号
		aversion 当前数据节点ACL变更版本号
	悲观锁和乐观锁:
		悲观锁:具有强烈的排他性,在当前事务结束之前,下一个事务不能访问相同的资源
		乐观锁:通过版本号来控制,当修改数据时验证版本号,如果数据已经修改过则抛出异常让客户端自行处理
	
8.hbase过滤器
	SingleColumnValueFilter  列值过滤器 SingleColumnValueFilter("列簇","列名",CompareOp(比较方法,大于小于等,枚举值),"值") 
	RowFilter             ROWKEY过滤器 支持正则表达式过滤器	RowFilter(CompareOp(比较方法,大于小于等,枚举值),ByteArrayComparable(该类的子类,支持正则表达式等))
	ValueFilter			值过滤器不需要指定列	ValueFilter(CompareOp(比较方法,大于小于等,枚举值),ByteArrayComparable(该类的子类,支持正则表达式等))
	QualifierFilter		列名过滤器	QualifierFilter(CompareOp(比较方法,大于小于等,枚举值),ByteArrayComparable(该类的子类,支持正则表达式等))
	PrefixFilter		前缀过滤器	前缀过滤器将会过滤掉不匹配的记录，过滤的对象是主键的值.
	PageFilter			页过滤器		页过滤器可以根据主键有序返回固定数量的记录,这需要客户端在遍历的时候记住页开始的地方,配合scan的startkey一起使用.
	KeyOnlyFilter		键过滤器		键过滤器可以简单的设置过滤的结果集中只包含键而忽略值,这里有一个选项可以把结果集的值保存为值的长度.
	FirstKeyOnlyFilter	键过滤器		在键过滤器的基础上,根据列有序,只包含第一个满足的键.
	ColumnPrefixFilter	列值过滤器	这里过滤的对象是列的值.
	TimestampsFilter	版本过滤器	这里参数是一个集合,只有包含在集合中的版本才会包含在结果集中.
	SkipFilter			包装过滤器	此类过滤器要通过包装其他的过滤器才有意义,是其他过滤器的一种加强.
	FilterList			过滤器集合	FilterList(FilterList.Operator(枚举值,说明结果集的AND和OR的关系)).
	Counter				计数器		
9.Hbase相关笔记
	hbase是三维有序的,主键有序,列有序,版本有序
	
	
10.Hbase shell

11.spark
	spark-submit提交时的选项
	--total-executor-cores:总共用多少个线程执行
	--class:指定启动类
	--master:指定master节点 

12.大数据环境部署工具:Ambari(可以方便的自动部署分布式环境)